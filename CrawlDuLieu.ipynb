import requests
import time
import csv

# Đường dẫn đến file CSV
file_path = r"E:\\DA\\Crawl\\29_NgheAn.csv"

# Mở file CSV để ghi dữ liệu
with open(file_path, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    
    # Ghi tiêu đề cột
    writer.writerow([
        'sbd', 'toan', 'ngu_van', 'ngoai_ngu', 'vat_li', 'hoa_hoc', 'sinh_hoc',
        'TBKHTN', 'lich_su', 'dia_li', 'gdcd', 'TBKHXH'
    ])
    
    # Vòng lặp lấy dữ liệu
    for x in range(29000001, 29036957):
        scraping_url = f"https://dantri.com.vn/thpt/1/0/99/{x}/2024/0.2/search-gradle.htm"
        
        try:
            # Gửi yêu cầu
            response = requests.get(scraping_url)
            response.raise_for_status()  # Raise an HTTPError for bad responses
            
            # Phân tích phản hồi JSON
            data = response.json()
            
            # Kiểm tra nếu có khóa 'student' trong phản hồi
            if 'student' in data:
                info = data['student']
                
                # Ghi dữ liệu vào file CSV
                writer.writerow([
                    info.get('sbd', 'None'),
                    info.get('toan', 'None'),
                    info.get('van', 'None'),
                    info.get('ngoaiNgu', 'None'),
                    info.get('vatLy', 'None'),
                    info.get('hoaHoc', 'None'),
                    info.get('sinhHoc', 'None'),
                    info.get('diemTBTuNhien', 'None'),
                    info.get('lichSu', 'None'),
                    info.get('diaLy', 'None'),
                    info.get('gdcd', 'None'),
                    info.get('diemTBXaHoi', 'None')
                ])
                
                # Hiển thị thông báo đã ghi dữ liệu thành công
                print(f"Đã ghi dữ liệu thành công cho SBD: {info.get('sbd', 'Unknown')}")

                # Đảm bảo dữ liệu được ghi vào tệp ngay lập tức
                file.flush()
            else:
                print(f"Không có khóa 'student' trong phản hồi cho URL: {scraping_url}")
            
        except requests.RequestException as e:
            print(f"Yêu cầu không thành công cho URL {scraping_url}: {e}")
        except ValueError as e:
            print(f"Lỗi khi phân tích cú pháp JSON cho URL {scraping_url}: {e}")
        except csv.Error as e:
            print(f"Lỗi khi ghi vào CSV cho URL {scraping_url}: {e}")
        
        # Thêm thời gian chờ 0.5 giây giữa các yêu cầu
        time.sleep(0.4)
